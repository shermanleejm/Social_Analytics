{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitf7dc2741bafe4a9b89002a7641fa85c3",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "swords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                    name                                            message  \\\n39924         Maplestori  Next time order 鱼柳包，then say 给我鸡胸肉 = no bone a...   \n39925           is_10yrs                You want filet-o-fish.... boneless?   \n39926            zesponk                   are we even sure its fish bones?   \n39927          prime5119                                                鱼胸肉   \n39928  7_92x57_mm_Mauser                             Could be plastic bones   \n\n                 timestamp  \n39924  2020-03-08 10:03:59  \n39925  2020-03-08 09:59:43  \n39926  2020-03-08 00:53:08  \n39927  2020-03-08 10:06:36  \n39928  2020-03-08 00:55:01  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>message</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>39924</th>\n      <td>Maplestori</td>\n      <td>Next time order 鱼柳包，then say 给我鸡胸肉 = no bone a...</td>\n      <td>2020-03-08 10:03:59</td>\n    </tr>\n    <tr>\n      <th>39925</th>\n      <td>is_10yrs</td>\n      <td>You want filet-o-fish.... boneless?</td>\n      <td>2020-03-08 09:59:43</td>\n    </tr>\n    <tr>\n      <th>39926</th>\n      <td>zesponk</td>\n      <td>are we even sure its fish bones?</td>\n      <td>2020-03-08 00:53:08</td>\n    </tr>\n    <tr>\n      <th>39927</th>\n      <td>prime5119</td>\n      <td>鱼胸肉</td>\n      <td>2020-03-08 10:06:36</td>\n    </tr>\n    <tr>\n      <th>39928</th>\n      <td>7_92x57_mm_Mauser</td>\n      <td>Could be plastic bones</td>\n      <td>2020-03-08 00:55:01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df = pd.read_json(\"./data/data71.json\")\n",
    "df1 = pd.read_json(\"./data/data70.json\")\n",
    "\n",
    "df = pd.concat([df, df1])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nThe Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. \\nThis means our sentence was rated as 67% Positive, 33% Neutral and 0% Negative. Hence all these should add up to 1.\\nThe Compound score is a metric that calculates the sum of all the lexicon ratings \\nwhich have been normalized between -1(most extreme negative) and +1 (most extreme positive).\\n'"
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "universities = [\n",
    "    \"Singapore Management University|smu\",\n",
    "    \"nanyang technological university|ntu\",\n",
    "    \"National University of Singapore|nus\",\n",
    "    \"Singapore University of Technology and Design|sutd\"\n",
    "]\n",
    "\n",
    "infoSys = \"information systems|it|information technology|data analytics|computer science|com|computer|info systems|info sys\"\n",
    "simplifiedInfoSys = \"computer|com|infosys|information systems|analytics\"\n",
    "simplifiedJC = \"rank|a-level|a level|\"\n",
    "\n",
    "polyjc = []\n",
    "with open(\"./data/polyjc.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        polyjc.append(line.strip()) \n",
    "\n",
    "def getScore(sentence): # return dict, e.g. --> {'neg': 0.0, 'neu': 0.326, 'pos': 0.674, 'compound': 0.7351}\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    return analyser.polarity_scores(sentence)\n",
    "\n",
    "def writeFile(filepath, header, uni, startYear, endYear, queryRegexArray, domain=\"\"):\n",
    "    df = pd.read_json(\"./data/data71.json\")\n",
    "    # df1 = pd.read_json(\"./data/data70.json\")\n",
    "    # df = pd.concat([df, df1])\n",
    "\n",
    "    f = open(filepath, \"w+\")\n",
    "    f.write(header)\n",
    "    for year in range(startYear, endYear + 1):\n",
    "        dfYear = df[df[\"timestamp\"].str.contains(str(year))]\n",
    "        yearStr = str(year)\n",
    "        for term in queryRegexArray:\n",
    "            if domain != \"\" :\n",
    "                term += \"|\" + domain\n",
    "            listOfComments = dfYear[ dfYear[\"message\"].str.contains(term, case=False) & dfYear[\"message\"].str.contains(uni, case=False)][\"message\"].values.tolist()\n",
    "            score = 0\n",
    "            numOfComments = len(listOfComments) \n",
    "            for comment in listOfComments:\n",
    "                score += getScore(comment)[\"compound\"]\n",
    "            overallScore = 0\n",
    "            if numOfComments != 0:\n",
    "                overallScore = score / numOfComments\n",
    "            yearStr += \",\" + str(overallScore)\n",
    "        f.write(yearStr)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "def printWeightiestSentences(filepath, header, uni, startYear, endYear, queryRegexArray, numOfSentences, domain=\"\"):\n",
    "    df = pd.read_json(\"./data/data70.json\")\n",
    "    df1 = pd.read_json(\"./data/data71.json\")\n",
    "    df = pd.concat([df, df1])\n",
    "    print (df.info())\n",
    "    f = open(filepath, \"w+\")\n",
    "    f.write(header)\n",
    "\n",
    "    for year in range(startYear, endYear + 1):\n",
    "        dfYear = df[df[\"timestamp\"].str.contains(str(year), na=False)]\n",
    "        for term in queryRegexArray:\n",
    "            if domain != \"\":\n",
    "                term = term + \"|\" + domain\n",
    "            listOfComments = dfYear[ dfYear[\"message\"].str.contains(term, case=False) & dfYear[\"message\"].str.contains(uni, case=False) ][\"message\"].values.tolist()\n",
    "\n",
    "            vectorizer = TfidfVectorizer(stop_words=swords)\n",
    "\n",
    "            try :\n",
    "                X = vectorizer.fit_transform(listOfComments)\n",
    "\n",
    "                feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "                vocab = vectorizer.vocabulary_\n",
    "\n",
    "                unsorted_result = {}\n",
    "\n",
    "                for i in range(len(list(X.toarray()))) :\n",
    "                    row = list(list(X.toarray())[i])\n",
    "                    unsorted_result[listOfComments[i]] = sum(row)\n",
    "                \n",
    "                result = pd.DataFrame()\n",
    "                result[\"sentence\"] = unsorted_result.keys()\n",
    "                result[\"value\"] = unsorted_result.values()\n",
    "\n",
    "                print (result.head())\n",
    "\n",
    "                df = result.sort_values(by=[\"value\"], ascending=False)\n",
    "                top10sentences = df.nlargest(numOfSentences, \"value\")[\"sentence\"].tolist()\n",
    "                top10values = df.nlargest(numOfSentences, \"value\")[\"value\"].tolist()\n",
    "                if len(top10sentences) >= numOfSentences:\n",
    "                    for i in range(numOfSentences):\n",
    "                        line = str(year) + \",\" + uni + \",\" + term + \",\\\"\" + top10sentences[i] + \"\\\",\" + str(top10values[i]) + \"\\n\"\n",
    "                        f.write(line)\n",
    "                        \n",
    "                elif len(top10sentences) > 0:\n",
    "                    for i in range(len(top10sentences)):\n",
    "                        line = str(year) + \",\" + uni + \",\" + term + \",\\\"\" + top10sentences[i] + \"\\\",\" + str(top10values[i]) + \"\\n\"\n",
    "                        f.write(line)\n",
    "\n",
    "                else:\n",
    "                    line = str(year) + \",\" + uni + \",\" + term + \",\" + \",\\n\"\n",
    "                    f.write(line)\n",
    "\n",
    "            except:\n",
    "                line = str(year) + \",\" + uni + \",\" + term + \",\" + \",\\n\"\n",
    "                f.write(line)\n",
    "\n",
    "\n",
    "    f.close()\n",
    "\n",
    "'''\n",
    "The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. \n",
    "This means our sentence was rated as 67% Positive, 33% Neutral and 0% Negative. Hence all these should add up to 1.\n",
    "The Compound score is a metric that calculates the sum of all the lexicon ratings \n",
    "which have been normalized between -1(most extreme negative) and +1 (most extreme positive).\n",
    "'''\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 82417 entries, 0 to 42487\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   name       82417 non-null  object\n 1   message    82417 non-null  object\n 2   timestamp  82417 non-null  object\ndtypes: object(3)\nmemory usage: 2.5+ MB\nNone\n                                            sentence     value\n0  I took Biz Finance BU8201 The killer part is t...  5.503282\n1  wargoku wrote:  I took Biz Finance BU8201 The ...  6.667480\n2  The original module in NBS (AB1201 - Financial...  6.534709\n3  heenny wrote:  Appeal failed -.- Surprisingly,...  3.155357\n4  Liao123 wrote:  Surprisingly, SMU is still pro...  2.995494\n                                            sentence      value\n0  Jackel_Yang wrote:  Wa Weaboo didi so kan chio...   7.344715\n1  Darkzi0n wrote:  bcos final gpa or o lvl score...  12.260953\n2  Irwin951 wrote:  Oh, are you from SMU? I have ...   5.871274\n3  Zabiyaki wrote:  This one is ugrad thread le.....   6.881803\n4  orzzwh wrote:  HiHi,  What are the chances for...   6.345578\n                                            sentence     value\n0  Ngee Ann Poly Accountancy 5 Sem GPA 3.69 O Lev...  3.838107\n1  applexn12 wrote:  Ngee Ann Poly Accountancy 5 ...  9.310092\n2  Hi guys!  I'm currently studying in TP's Busin...  7.691105\n3  Remedy95 wrote:  Hi guys!  I'm currently study...  9.005578\n4  Smartiesailor wrote:  I applied for NTU mariti...  4.959115\n                                            sentence     value\n0  Jackel_Yang wrote:  Wa Weaboo didi so kan chio...  7.464492\n1  Hi guys!  I'm currently studying in TP's Busin...  7.823852\n2  Remedy95 wrote:  Hi guys!  I'm currently study...  9.137801\n3  PetPet wrote:  hello SMU students! I've made a...  4.818435\n4  PetPet wrote:  hello SMU students! I've made a...  4.479154\n                                            sentence      value\n0  Just to check... Poly:NYP Course:molecular bio...   3.253895\n1  ying peng wrote:  Hi, I finish diploma in nyp ...   7.218082\n2  ying peng wrote:  Hi, I finish diploma in nyp ...  10.167994\n3  Hi, I finish diploma in nyp last year, and cos...   6.717321\n4  ying peng wrote:  Hi, I finish diploma in nyp ...   7.060280\n"
    }
   ],
   "source": [
    "filepath = \"./output/tfidf.csv\"\n",
    "header = \"year,uni,search term,sentence,tfidf value\\n\"\n",
    "uni = \"Singapore Management University|smu\"\n",
    "start = 2014\n",
    "end = 2014\n",
    "arr = polyjc\n",
    "num = 10\n",
    "\n",
    "printWeightiestSentences(filepath, header, uni, start, end, arr, num)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}